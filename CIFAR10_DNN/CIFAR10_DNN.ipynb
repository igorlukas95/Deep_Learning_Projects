{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cooperative-nelson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-kruger",
   "metadata": {},
   "source": [
    "Build DNN with 20 hidden layers of 100 neurons each. Use He initialization and the ELU activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-pasta",
   "metadata": {},
   "source": [
    "# Build DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hindu-tribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32,32,3]))\n",
    "\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                activation='elu',\n",
    "                                kernel_initializer='he_normal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-labor",
   "metadata": {},
   "source": [
    "Using Nadam optimizer and early stopping train the network on the CIFAR10 dataset. The datset is composed of 60 000 32 x 32 pixel color images ( 50000 for learning and 10000 for testing) with 10 classes. We use softmax activation output layer. We serch for right learning rate each time we change model hyperparametrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "blind-algeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(10, activation = 'softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-columbia",
   "metadata": {},
   "source": [
    "Use Nadam optimizer with lr of 5e-5. After comparing them in Tensorboard this setup looks better than others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "identical-period",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(lr= 5e-5)\n",
    "model.compile(loss = 'sparse_categorical_crossentropy',\n",
    "             optimizer = optimizer,\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-intranet",
   "metadata": {},
   "source": [
    "# Loading CIFAR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "later-groove",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 47s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "X_train = X_train_full[5000:]\n",
    "y_train = y_train_full[5000:]\n",
    "X_valid = X_train_full[:5000]\n",
    "y_valid = y_train_full[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-translation",
   "metadata": {},
   "source": [
    "Building callbacks we need to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "finnish-village",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint('my_cifar10_model.h5', save_best_only=True)\n",
    "run_index = 1 # Increment every time you train model\n",
    "run_logdir = os.path.join(os.curdir, 'my_cifar10_logs', 'run){:03d}'.format(run_index))\n",
    "tensorboad_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboad_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "worthy-pierre",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 9664."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_cifar10_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "supreme-particle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 15s 9ms/step - loss: 9.6474 - accuracy: 0.1398 - val_loss: 2.1317 - val_accuracy: 0.2358\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 11s 7ms/step - loss: 2.0824 - accuracy: 0.2412 - val_loss: 2.0609 - val_accuracy: 0.2504\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.9496 - accuracy: 0.2886 - val_loss: 1.9954 - val_accuracy: 0.2706\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.8545 - accuracy: 0.3231 - val_loss: 1.8433 - val_accuracy: 0.3388\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.7888 - accuracy: 0.3455 - val_loss: 1.7957 - val_accuracy: 0.3438\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.7450 - accuracy: 0.3654 - val_loss: 1.8352 - val_accuracy: 0.3418\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.6999 - accuracy: 0.3828 - val_loss: 1.7427 - val_accuracy: 0.3654\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 11s 7ms/step - loss: 1.6662 - accuracy: 0.3968 - val_loss: 1.6768 - val_accuracy: 0.3960\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.6391 - accuracy: 0.4035 - val_loss: 1.6467 - val_accuracy: 0.4044\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.6102 - accuracy: 0.4181 - val_loss: 1.6611 - val_accuracy: 0.3910\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.5813 - accuracy: 0.4299 - val_loss: 1.6566 - val_accuracy: 0.4064\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.5648 - accuracy: 0.4353 - val_loss: 1.6832 - val_accuracy: 0.4058\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.5399 - accuracy: 0.4486 - val_loss: 1.6008 - val_accuracy: 0.4314\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.5148 - accuracy: 0.4525 - val_loss: 1.5818 - val_accuracy: 0.4358\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.5128 - accuracy: 0.4519 - val_loss: 1.5803 - val_accuracy: 0.4334\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.4919 - accuracy: 0.4643 - val_loss: 1.5684 - val_accuracy: 0.4394\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.4782 - accuracy: 0.4712 - val_loss: 1.6029 - val_accuracy: 0.4260\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.4585 - accuracy: 0.4758 - val_loss: 1.5617 - val_accuracy: 0.4448\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.4385 - accuracy: 0.4808 - val_loss: 1.5482 - val_accuracy: 0.4494\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.4361 - accuracy: 0.4790 - val_loss: 1.6002 - val_accuracy: 0.4240\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.4216 - accuracy: 0.4874 - val_loss: 1.5779 - val_accuracy: 0.4344\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.4077 - accuracy: 0.4919 - val_loss: 1.5460 - val_accuracy: 0.4514\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.3921 - accuracy: 0.5011 - val_loss: 1.5771 - val_accuracy: 0.4404\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.3819 - accuracy: 0.4995 - val_loss: 1.5460 - val_accuracy: 0.4536\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.3751 - accuracy: 0.5025 - val_loss: 1.5139 - val_accuracy: 0.4704\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.3493 - accuracy: 0.5087 - val_loss: 1.5493 - val_accuracy: 0.4586\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.3525 - accuracy: 0.5109 - val_loss: 1.5056 - val_accuracy: 0.4678\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.3363 - accuracy: 0.5187 - val_loss: 1.5730 - val_accuracy: 0.4520\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.3405 - accuracy: 0.5133 - val_loss: 1.5024 - val_accuracy: 0.4774\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.3322 - accuracy: 0.5193 - val_loss: 1.5477 - val_accuracy: 0.4714\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.3298 - accuracy: 0.5196 - val_loss: 1.5844 - val_accuracy: 0.4388\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.2996 - accuracy: 0.5314 - val_loss: 1.5269 - val_accuracy: 0.4666\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.3013 - accuracy: 0.5298 - val_loss: 1.5284 - val_accuracy: 0.4752\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.2855 - accuracy: 0.5355 - val_loss: 1.5421 - val_accuracy: 0.4718\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.2812 - accuracy: 0.5350 - val_loss: 1.5501 - val_accuracy: 0.4672\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.2709 - accuracy: 0.5434 - val_loss: 1.4950 - val_accuracy: 0.4806\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.2614 - accuracy: 0.5414 - val_loss: 1.5052 - val_accuracy: 0.4848\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.2520 - accuracy: 0.5509 - val_loss: 1.5249 - val_accuracy: 0.4758\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.2509 - accuracy: 0.5508 - val_loss: 1.5538 - val_accuracy: 0.4626\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.2408 - accuracy: 0.5535 - val_loss: 1.5250 - val_accuracy: 0.4722\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.2322 - accuracy: 0.5557 - val_loss: 1.5368 - val_accuracy: 0.4694\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.2232 - accuracy: 0.5562 - val_loss: 1.5266 - val_accuracy: 0.4746\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.2238 - accuracy: 0.5621 - val_loss: 1.5504 - val_accuracy: 0.4640\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.2070 - accuracy: 0.5680 - val_loss: 1.5518 - val_accuracy: 0.4632\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.2050 - accuracy: 0.5635 - val_loss: 1.5628 - val_accuracy: 0.4778\n",
      "Epoch 46/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1864 - accuracy: 0.5752 - val_loss: 1.5106 - val_accuracy: 0.4864\n",
      "Epoch 47/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1825 - accuracy: 0.5726 - val_loss: 1.5453 - val_accuracy: 0.4824\n",
      "Epoch 48/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1870 - accuracy: 0.5729 - val_loss: 1.5454 - val_accuracy: 0.4770\n",
      "Epoch 49/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1824 - accuracy: 0.5752 - val_loss: 1.5294 - val_accuracy: 0.4794\n",
      "Epoch 50/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1634 - accuracy: 0.5807 - val_loss: 1.5587 - val_accuracy: 0.4848\n",
      "Epoch 51/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1623 - accuracy: 0.5790 - val_loss: 1.5355 - val_accuracy: 0.4848\n",
      "Epoch 52/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1557 - accuracy: 0.5846 - val_loss: 1.5678 - val_accuracy: 0.4784\n",
      "Epoch 53/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1507 - accuracy: 0.5854 - val_loss: 1.5895 - val_accuracy: 0.4694\n",
      "Epoch 54/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1393 - accuracy: 0.5847 - val_loss: 1.6177 - val_accuracy: 0.4650\n",
      "Epoch 55/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1406 - accuracy: 0.5926 - val_loss: 1.5664 - val_accuracy: 0.4804\n",
      "Epoch 56/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1240 - accuracy: 0.5954 - val_loss: 1.5610 - val_accuracy: 0.4804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20f99596940>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "compound-light",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 2ms/step - loss: 1.4950 - accuracy: 0.4806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4949904680252075, 0.4805999994277954]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model('my_cifar10_model.h5')\n",
    "model.evaluate(X_valid,y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-spell",
   "metadata": {},
   "source": [
    "The model with lowest validation loss gets about 48 % accuracy on validation set. It took 27 epochs to reach the lowest validation loss, with roughly 8 seconds per epoch on my laptop (without GPU). Let's improve performance with Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-artwork",
   "metadata": {},
   "source": [
    "BN layers are added after Every Dense layer exept the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "transparent-mandate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 37s 19ms/step - loss: 1.9809 - accuracy: 0.2925 - val_loss: 1.6815 - val_accuracy: 0.4038\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.6830 - accuracy: 0.3982 - val_loss: 1.5858 - val_accuracy: 0.4322\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.6160 - accuracy: 0.4265 - val_loss: 1.5552 - val_accuracy: 0.4422\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.5573 - accuracy: 0.4445 - val_loss: 1.5626 - val_accuracy: 0.4470\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.5114 - accuracy: 0.4614 - val_loss: 1.4501 - val_accuracy: 0.4770\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.4684 - accuracy: 0.4755 - val_loss: 1.4182 - val_accuracy: 0.4886\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.4341 - accuracy: 0.4892 - val_loss: 1.3961 - val_accuracy: 0.5022\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.4089 - accuracy: 0.5011 - val_loss: 1.3766 - val_accuracy: 0.5130\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.3819 - accuracy: 0.5107 - val_loss: 1.3634 - val_accuracy: 0.5126\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.3575 - accuracy: 0.5202 - val_loss: 1.3506 - val_accuracy: 0.5238\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.3310 - accuracy: 0.5293 - val_loss: 1.3469 - val_accuracy: 0.5124\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.3144 - accuracy: 0.5352 - val_loss: 1.3822 - val_accuracy: 0.5018\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.2927 - accuracy: 0.5455 - val_loss: 1.3476 - val_accuracy: 0.5200\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.2658 - accuracy: 0.5509 - val_loss: 1.3577 - val_accuracy: 0.5258\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.2593 - accuracy: 0.5538 - val_loss: 1.3880 - val_accuracy: 0.5104\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.2493 - accuracy: 0.5565 - val_loss: 1.3541 - val_accuracy: 0.5248\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.2298 - accuracy: 0.5637 - val_loss: 1.3287 - val_accuracy: 0.5312\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.2134 - accuracy: 0.5712 - val_loss: 1.3512 - val_accuracy: 0.5240\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.1991 - accuracy: 0.5802 - val_loss: 1.3427 - val_accuracy: 0.5298\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.1875 - accuracy: 0.5804 - val_loss: 1.3995 - val_accuracy: 0.5124\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1790 - accuracy: 0.5784 - val_loss: 1.3571 - val_accuracy: 0.5214\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.1559 - accuracy: 0.5900 - val_loss: 1.3599 - val_accuracy: 0.5270\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.1404 - accuracy: 0.5997 - val_loss: 1.3254 - val_accuracy: 0.5384\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.1300 - accuracy: 0.6035 - val_loss: 1.3192 - val_accuracy: 0.5384\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.1182 - accuracy: 0.6083 - val_loss: 1.3413 - val_accuracy: 0.5350\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.1056 - accuracy: 0.6091 - val_loss: 1.3399 - val_accuracy: 0.5324\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.0850 - accuracy: 0.6153 - val_loss: 1.3456 - val_accuracy: 0.5324\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.0847 - accuracy: 0.6200 - val_loss: 1.3522 - val_accuracy: 0.5266\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.0770 - accuracy: 0.6192 - val_loss: 1.3335 - val_accuracy: 0.5360\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.0722 - accuracy: 0.6191 - val_loss: 1.3528 - val_accuracy: 0.5304\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.0657 - accuracy: 0.6218 - val_loss: 1.3560 - val_accuracy: 0.5356\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.0355 - accuracy: 0.6311 - val_loss: 1.3648 - val_accuracy: 0.5398\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.0282 - accuracy: 0.6341 - val_loss: 1.3478 - val_accuracy: 0.5480\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.0220 - accuracy: 0.6404 - val_loss: 1.3612 - val_accuracy: 0.5340\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.0157 - accuracy: 0.6439 - val_loss: 1.3464 - val_accuracy: 0.5406\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 0.9995 - accuracy: 0.6472 - val_loss: 1.3438 - val_accuracy: 0.5424\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 0.9808 - accuracy: 0.6529 - val_loss: 1.3560 - val_accuracy: 0.5352\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 0.9765 - accuracy: 0.6548 - val_loss: 1.3642 - val_accuracy: 0.5382\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 0.9802 - accuracy: 0.6543 - val_loss: 1.3860 - val_accuracy: 0.5316\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 0.9602 - accuracy: 0.6607 - val_loss: 1.3947 - val_accuracy: 0.5380\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 0.9545 - accuracy: 0.6634 - val_loss: 1.3813 - val_accuracy: 0.5396\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.9521 - accuracy: 0.6633 - val_loss: 1.3825 - val_accuracy: 0.5372\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.9376 - accuracy: 0.6677 - val_loss: 1.3757 - val_accuracy: 0.5396\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 0.9371 - accuracy: 0.6694 - val_loss: 1.4013 - val_accuracy: 0.5318\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 1.3192 - accuracy: 0.5384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3191810846328735, 0.5383999943733215]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32,32,3]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation('elu'))\n",
    "model.add(keras.layers.Dense(10,activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(lr=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_bn_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_bn_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_bn_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-converter",
   "metadata": {},
   "source": [
    "The model is converging faster than before. The previous model took 27 epochs to reach the lowest validation loss, while the new model achieved that same loss in just 5 epochs and continued to make progress until the 16th epoch. The BN layers stabilized training and allowed us to use a much larger learning rate, so convergence was faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-tuition",
   "metadata": {},
   "source": [
    "BN produce a better model - the final model converged with much better accuray 54% instead of 48 %. It's still not very good but much better than before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-spring",
   "metadata": {},
   "source": [
    "BN model converge much faster than without model. Each epoch required more time but overall there are less epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-intensity",
   "metadata": {},
   "source": [
    "# Replacing BN with SELU and other adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-stupid",
   "metadata": {},
   "source": [
    "Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "automotive-schedule",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 18s 11ms/step - loss: 2.2782 - accuracy: 0.1818 - val_loss: 2.1038 - val_accuracy: 0.2260\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.9494 - accuracy: 0.2750 - val_loss: 2.7958 - val_accuracy: 0.1792\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.9050 - accuracy: 0.2924 - val_loss: 1.9841 - val_accuracy: 0.2888\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.8518 - accuracy: 0.3184 - val_loss: 2.0070 - val_accuracy: 0.2806\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.8163 - accuracy: 0.3322 - val_loss: 1.8091 - val_accuracy: 0.3308\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.7824 - accuracy: 0.3441 - val_loss: 1.8567 - val_accuracy: 0.3354\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.7640 - accuracy: 0.3504 - val_loss: 1.7226 - val_accuracy: 0.3766\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.7376 - accuracy: 0.3624 - val_loss: 1.7568 - val_accuracy: 0.3528\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.7747 - accuracy: 0.3510 - val_loss: 1.8465 - val_accuracy: 0.3146\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.8214 - accuracy: 0.3208 - val_loss: 1.7818 - val_accuracy: 0.3352\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.7734 - accuracy: 0.3394 - val_loss: 1.8737 - val_accuracy: 0.3062\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.7742 - accuracy: 0.3477 - val_loss: 1.7702 - val_accuracy: 0.3468\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.7566 - accuracy: 0.3513 - val_loss: 1.8249 - val_accuracy: 0.3408\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.7496 - accuracy: 0.3566 - val_loss: 1.7683 - val_accuracy: 0.3248\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.7378 - accuracy: 0.3589 - val_loss: 1.7281 - val_accuracy: 0.3684\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.7223 - accuracy: 0.3649 - val_loss: 1.7431 - val_accuracy: 0.3532\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.7021 - accuracy: 0.3700 - val_loss: 1.7474 - val_accuracy: 0.3622\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.6917 - accuracy: 0.3781 - val_loss: 1.7664 - val_accuracy: 0.3612\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.6771 - accuracy: 0.3855 - val_loss: 1.6887 - val_accuracy: 0.3862\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6649 - accuracy: 0.3922 - val_loss: 1.7248 - val_accuracy: 0.3674\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6930 - accuracy: 0.3779 - val_loss: 1.8794 - val_accuracy: 0.2898\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.7927 - accuracy: 0.3270 - val_loss: 1.7464 - val_accuracy: 0.3468\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.7324 - accuracy: 0.3533 - val_loss: 1.7649 - val_accuracy: 0.3398\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.7052 - accuracy: 0.3660 - val_loss: 1.7432 - val_accuracy: 0.3676\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.6976 - accuracy: 0.3758 - val_loss: 1.7178 - val_accuracy: 0.3726\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.6660 - accuracy: 0.3892 - val_loss: 1.8333 - val_accuracy: 0.3040\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6915 - accuracy: 0.3781 - val_loss: 1.7203 - val_accuracy: 0.3702\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6493 - accuracy: 0.4003 - val_loss: 1.6901 - val_accuracy: 0.3836\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6445 - accuracy: 0.3979 - val_loss: 1.6845 - val_accuracy: 0.3952\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6405 - accuracy: 0.4075 - val_loss: 1.7223 - val_accuracy: 0.3704\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6367 - accuracy: 0.4099 - val_loss: 1.7246 - val_accuracy: 0.3766\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6169 - accuracy: 0.4124 - val_loss: 1.6783 - val_accuracy: 0.3976\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6260 - accuracy: 0.4133 - val_loss: 1.6961 - val_accuracy: 0.3904\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6079 - accuracy: 0.4205 - val_loss: 1.6878 - val_accuracy: 0.3938\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6092 - accuracy: 0.4123 - val_loss: 1.6813 - val_accuracy: 0.3840\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6068 - accuracy: 0.4224 - val_loss: 1.6532 - val_accuracy: 0.4098\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6018 - accuracy: 0.4236 - val_loss: 1.6674 - val_accuracy: 0.4080\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.5790 - accuracy: 0.4280 - val_loss: 1.6629 - val_accuracy: 0.3950\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.5894 - accuracy: 0.4253 - val_loss: 1.6703 - val_accuracy: 0.3946\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5802 - accuracy: 0.4313 - val_loss: 1.6597 - val_accuracy: 0.4018\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.7455 - accuracy: 0.4046 - val_loss: 1.7219 - val_accuracy: 0.3690\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6765 - accuracy: 0.3899 - val_loss: 1.6982 - val_accuracy: 0.3802\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6549 - accuracy: 0.3934 - val_loss: 1.7090 - val_accuracy: 0.3894\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6430 - accuracy: 0.3975 - val_loss: 1.7043 - val_accuracy: 0.3802\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6335 - accuracy: 0.4063 - val_loss: 1.7015 - val_accuracy: 0.3894\n",
      "Epoch 46/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6269 - accuracy: 0.4082 - val_loss: 1.6901 - val_accuracy: 0.3904\n",
      "Epoch 47/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6282 - accuracy: 0.4085 - val_loss: 1.6984 - val_accuracy: 0.3838\n",
      "Epoch 48/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6173 - accuracy: 0.4115 - val_loss: 1.6865 - val_accuracy: 0.3918\n",
      "Epoch 49/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6076 - accuracy: 0.4181 - val_loss: 1.6364 - val_accuracy: 0.4054\n",
      "Epoch 50/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5830 - accuracy: 0.4289 - val_loss: 1.6503 - val_accuracy: 0.4164\n",
      "Epoch 51/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5825 - accuracy: 0.4278 - val_loss: 1.6487 - val_accuracy: 0.3970\n",
      "Epoch 52/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5774 - accuracy: 0.4287 - val_loss: 1.6894 - val_accuracy: 0.3926\n",
      "Epoch 53/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5722 - accuracy: 0.4352 - val_loss: 1.9298 - val_accuracy: 0.2822\n",
      "Epoch 54/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.7660 - accuracy: 0.3507 - val_loss: 1.7055 - val_accuracy: 0.3766\n",
      "Epoch 55/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6741 - accuracy: 0.3915 - val_loss: 1.6798 - val_accuracy: 0.3968\n",
      "Epoch 56/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6343 - accuracy: 0.4058 - val_loss: 1.7091 - val_accuracy: 0.3866\n",
      "Epoch 57/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6199 - accuracy: 0.4124 - val_loss: 1.6597 - val_accuracy: 0.4084\n",
      "Epoch 58/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 85.6725 - accuracy: 0.4182 - val_loss: 1.8452 - val_accuracy: 0.3158\n",
      "Epoch 59/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.7373 - accuracy: 0.3553 - val_loss: 1.7179 - val_accuracy: 0.3796\n",
      "Epoch 60/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.6652 - accuracy: 0.3914 - val_loss: 1.6824 - val_accuracy: 0.3944\n",
      "Epoch 61/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6152 - accuracy: 0.4078 - val_loss: 1.6551 - val_accuracy: 0.3986\n",
      "Epoch 62/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5896 - accuracy: 0.4243 - val_loss: 1.6624 - val_accuracy: 0.4044\n",
      "Epoch 63/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5803 - accuracy: 0.4258 - val_loss: 1.6727 - val_accuracy: 0.3992\n",
      "Epoch 64/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6198 - accuracy: 0.4209 - val_loss: 1.6792 - val_accuracy: 0.3974\n",
      "Epoch 65/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5970 - accuracy: 0.4241 - val_loss: 1.6530 - val_accuracy: 0.4062\n",
      "Epoch 66/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5678 - accuracy: 0.4328 - val_loss: 1.6463 - val_accuracy: 0.4072\n",
      "Epoch 67/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5575 - accuracy: 0.4387 - val_loss: 1.6841 - val_accuracy: 0.4006\n",
      "Epoch 68/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5433 - accuracy: 0.4462 - val_loss: 1.6428 - val_accuracy: 0.4132\n",
      "Epoch 69/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5556 - accuracy: 0.4399 - val_loss: 1.6512 - val_accuracy: 0.4046\n",
      "157/157 [==============================] - 1s 2ms/step - loss: 1.6364 - accuracy: 0.4054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6363557577133179, 0.40540000796318054]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32,32,3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer=\"lecun_normal\",activation='selu'))\n",
    "model.add(keras.layers.Dense(10,activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(lr=7e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_selu_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_selu_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_selu_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-nickel",
   "metadata": {},
   "source": [
    "We get 47.9% accuracy, which is not much better than the original model (47.6%), and not as good as the model using batch normalization (54.0%). However, convergence was almost as fast as with the BN model, plus each epoch took only 7 seconds. So it's by far the fastest model to train so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-chair",
   "metadata": {},
   "source": [
    "# MC Dropout regularization alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "unauthorized-picture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 20s 10ms/step - loss: 2.0619 - accuracy: 0.2797 - val_loss: 1.7099 - val_accuracy: 0.3916\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.6787 - accuracy: 0.4044 - val_loss: 1.6869 - val_accuracy: 0.4120\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5846 - accuracy: 0.4504 - val_loss: 1.6547 - val_accuracy: 0.4206\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5014 - accuracy: 0.4723 - val_loss: 1.5722 - val_accuracy: 0.4564\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4448 - accuracy: 0.4937 - val_loss: 1.6269 - val_accuracy: 0.4520\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3977 - accuracy: 0.5085 - val_loss: 1.5222 - val_accuracy: 0.4878\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3470 - accuracy: 0.5322 - val_loss: 1.5798 - val_accuracy: 0.4820\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3072 - accuracy: 0.5476 - val_loss: 1.5083 - val_accuracy: 0.5018\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2767 - accuracy: 0.5563 - val_loss: 1.5165 - val_accuracy: 0.4914\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2435 - accuracy: 0.5704 - val_loss: 1.5222 - val_accuracy: 0.4952\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2087 - accuracy: 0.5867 - val_loss: 1.5860 - val_accuracy: 0.4916\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.1829 - accuracy: 0.5928 - val_loss: 1.5467 - val_accuracy: 0.4852\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.1506 - accuracy: 0.6050 - val_loss: 1.5738 - val_accuracy: 0.5006\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.1181 - accuracy: 0.6119 - val_loss: 1.5396 - val_accuracy: 0.5064\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.0978 - accuracy: 0.6260 - val_loss: 1.6322 - val_accuracy: 0.5124\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.0819 - accuracy: 0.6289 - val_loss: 1.6272 - val_accuracy: 0.5118\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.0541 - accuracy: 0.6394 - val_loss: 1.6474 - val_accuracy: 0.5078\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.0347 - accuracy: 0.6485 - val_loss: 1.5781 - val_accuracy: 0.5156\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.0056 - accuracy: 0.6588 - val_loss: 1.7217 - val_accuracy: 0.5052\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 0.9817 - accuracy: 0.6633 - val_loss: 1.6616 - val_accuracy: 0.5038\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 0.9666 - accuracy: 0.6716 - val_loss: 1.7078 - val_accuracy: 0.5162\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 0.9421 - accuracy: 0.6835 - val_loss: 1.6885 - val_accuracy: 0.5136\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 0.9222 - accuracy: 0.6868 - val_loss: 1.7138 - val_accuracy: 0.5122\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 0.9083 - accuracy: 0.6934 - val_loss: 1.7141 - val_accuracy: 0.5058\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 0.9023 - accuracy: 0.6949 - val_loss: 1.7199 - val_accuracy: 0.5196\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 0.8740 - accuracy: 0.7066 - val_loss: 1.7238 - val_accuracy: 0.5140\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 0.8560 - accuracy: 0.7101 - val_loss: 1.7109 - val_accuracy: 0.5144\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 0.8412 - accuracy: 0.7171 - val_loss: 1.7811 - val_accuracy: 0.4948\n",
      "157/157 [==============================] - 1s 2ms/step - loss: 1.5083 - accuracy: 0.5018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5082576274871826, 0.501800000667572]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32,32,3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer=\"lecun_normal\",activation='selu'))\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10,activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(lr=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_alpha_dropout_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_alpha_dropout_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_alpha_dropout_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-mentor",
   "metadata": {},
   "source": [
    "Let's use MC Dropout now. We will need the `MCAlphaDropout` class we used earlier, so let's just copy it here for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "contemporary-malawi",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-register",
   "metadata": {},
   "source": [
    "Now let's create a new model, identical to the one we just trained (with the same weights), but with `MCAlphaDropout` dropout layers instead of `AlphaDropout` layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "allied-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-jason",
   "metadata": {},
   "source": [
    "Than Lets add a couple utility functions. The first will run model many times (10 by default) and it will return the mean predicted class probabilities. The second will use these mean probabilities to predict the most likely class for each instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bored-norwegian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predict_probas(mc_model, X, n_samples = 10):\n",
    "    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n",
    "    return np.mean(Y_probas, axis = 0)\n",
    "\n",
    "def mc_dropout_predict_classes(mc_model, X, n_samples=10):\n",
    "    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n",
    "    return np.argmax(Y_probas, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-blair",
   "metadata": {},
   "source": [
    "Now let's make predictions for all the instances in the validation set, and compute the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "everyday-prince",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4996"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "y_pred = mc_dropout_predict_classes(mc_model, X_valid_scaled)\n",
    "accuracy = np.mean(y_pred == y_valid[:, 0])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-utilization",
   "metadata": {},
   "source": [
    "We get no accuracy improvement in this case\n",
    "So the best model is the Batch Normalization model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-cowboy",
   "metadata": {},
   "source": [
    "# 1 Cycle Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "productive-intent",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32,32,3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer=\"lecun_normal\",activation='selu'))\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10,activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=1e-3)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "moral-labor",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.lr))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n",
    "\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.lr)\n",
    "    K.set_value(model.optimizer.lr, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    K.set_value(model.optimizer.lr, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "professional-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.lr, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "becoming-diesel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 5s 10ms/step - loss: nan - accuracy: 0.1248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9.999999747378752e-06,\n",
       " 9.615227699279785,\n",
       " 2.617919683456421,\n",
       " 3.9362728595733647)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs = 1, batch_size=batch_size)\n",
    "plot_lr_vs_loss(rates, losses)\n",
    "plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) /1.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "static-philippines",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=1e-2)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fossil-delaware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "352/352 [==============================] - 5s 11ms/step - loss: 2.2238 - accuracy: 0.2404 - val_loss: 1.7889 - val_accuracy: 0.3792\n",
      "Epoch 2/15\n",
      "352/352 [==============================] - 3s 10ms/step - loss: 1.7876 - accuracy: 0.3695 - val_loss: 1.6318 - val_accuracy: 0.4262\n",
      "Epoch 3/15\n",
      "352/352 [==============================] - 3s 10ms/step - loss: 1.6445 - accuracy: 0.4172 - val_loss: 1.6232 - val_accuracy: 0.4252\n",
      "Epoch 4/15\n",
      "352/352 [==============================] - 3s 10ms/step - loss: 1.5478 - accuracy: 0.4482 - val_loss: 1.6383 - val_accuracy: 0.4256\n",
      "Epoch 5/15\n",
      "352/352 [==============================] - 3s 10ms/step - loss: 1.4876 - accuracy: 0.4742 - val_loss: 1.5952 - val_accuracy: 0.4448\n",
      "Epoch 6/15\n",
      "352/352 [==============================] - 3s 10ms/step - loss: 1.4422 - accuracy: 0.4873 - val_loss: 1.5819 - val_accuracy: 0.4522\n",
      "Epoch 7/15\n",
      "352/352 [==============================] - 3s 10ms/step - loss: 1.4121 - accuracy: 0.4955 - val_loss: 1.5613 - val_accuracy: 0.4660\n",
      "Epoch 8/15\n",
      "352/352 [==============================] - 4s 10ms/step - loss: 1.3485 - accuracy: 0.5202 - val_loss: 1.4980 - val_accuracy: 0.4918\n",
      "Epoch 9/15\n",
      "352/352 [==============================] - 4s 10ms/step - loss: 1.2729 - accuracy: 0.5465 - val_loss: 1.5257 - val_accuracy: 0.4910\n",
      "Epoch 10/15\n",
      "352/352 [==============================] - 3s 10ms/step - loss: 1.1943 - accuracy: 0.5722 - val_loss: 1.5377 - val_accuracy: 0.4964\n",
      "Epoch 11/15\n",
      "352/352 [==============================] - 3s 10ms/step - loss: 1.1210 - accuracy: 0.6012 - val_loss: 1.5484 - val_accuracy: 0.4942\n",
      "Epoch 12/15\n",
      "352/352 [==============================] - 4s 10ms/step - loss: 1.0619 - accuracy: 0.6202 - val_loss: 1.5105 - val_accuracy: 0.5140\n",
      "Epoch 13/15\n",
      "352/352 [==============================] - 4s 10ms/step - loss: 0.9884 - accuracy: 0.6456 - val_loss: 1.5470 - val_accuracy: 0.5224\n",
      "Epoch 14/15\n",
      "352/352 [==============================] - 3s 10ms/step - loss: 0.9173 - accuracy: 0.6743 - val_loss: 1.5571 - val_accuracy: 0.5310\n",
      "Epoch 15/15\n",
      "352/352 [==============================] - 3s 10ms/step - loss: 0.8834 - accuracy: 0.6852 - val_loss: 1.5883 - val_accuracy: 0.5288\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 15\n",
    "onecycle = OneCycleScheduler(math.ceil(len(X_train_scaled) / batch_size) * n_epochs, max_rate = 0.05)\n",
    "history = model.fit(X_train_scaled, y_train, epochs = n_epochs, batch_size= batch_size,\n",
    "                   validation_data=(X_valid_scaled, y_valid),\n",
    "                   callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-rolling",
   "metadata": {},
   "source": [
    "One cycle allowed us to train the model in just 15 epochs, each taking only 2 seconds (thanks to the larger batch size). This is several times faster than the fastest model we trained so far. Moreover, we improved the model's performance (from 47.6% to 52.0%). The batch normalized model reaches a slightly better performance (54%), but it's much slower to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-prediction",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
